{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx67IPL5Mnb4"
      },
      "source": [
        "## HW3: Decision Tree, AdaBoost and Random Forest\n",
        "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
        "\n",
        "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlCz7ZsFMnb8"
      },
      "source": [
        "## Question 1\n",
        "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "V7ShPNwgMnb9"
      },
      "outputs": [],
      "source": [
        "# Copy and paste your implementations right here to check your result\n",
        "# (Of course you can add your classes not written here)\n",
        "def gini(sequence):\n",
        "  frequency = {}\n",
        "  for label in sequence:\n",
        "    if label not in frequency:\n",
        "      frequency[label] = 0\n",
        "    frequency[label] += 1\n",
        "  gini_value = 1\n",
        "  \n",
        "  for freq in frequency.items():\n",
        "    gini_value -= (freq[1] / len(sequence)) ** 2 # 1 - pi^2 for all i\n",
        "  return gini_value\n",
        "      \n",
        "\n",
        "\n",
        "def entropy(sequence):\n",
        "  frequency = {}\n",
        "  for label in sequence:\n",
        "    if label not in frequency:\n",
        "      frequency[label] = 0\n",
        "    frequency[label] += 1\n",
        "  entropy_value = 0\n",
        "  for freq in frequency.items():\n",
        "    entropy_value -= np.log2(freq[1] / len(sequence)) * freq[1] / len(sequence) #-pi log pi for all i \n",
        "  return entropy_value\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_FV9VGZmMnb9"
      },
      "outputs": [],
      "source": [
        "# 1 = class 1,\n",
        "# 2 = class 2\n",
        "import numpy as np\n",
        "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5tYWdBCMnb-",
        "outputId": "ca26498b-c903-4a98-c2b2-e5e46d9a1e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gini of data is  0.4628099173553719\n"
          ]
        }
      ],
      "source": [
        "print(\"Gini of data is \", gini(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVUSqI2LMnb_",
        "outputId": "fcbcc0ad-4c9e-46c5-c5f3-32e67b82ddae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of data is  0.9456603046006402\n"
          ]
        }
      ],
      "source": [
        "print(\"Entropy of data is \", entropy(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZmwikyRMncA"
      },
      "source": [
        "## Load data\n",
        "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7wIjWKbMncA",
        "outputId": "85aad6fd-82f0-4426-deaa-83a60735c3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1200, 21)\n",
            "(300, 21)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "importance = [0 for i in range(20)]\n",
        "train_df = pd.read_csv('train.csv')\n",
        "val_df = pd.read_csv('val.csv')\n",
        "\n",
        "\n",
        "print(train_df.shape)\n",
        "print(val_df.shape)\n",
        "\n",
        "####split training data into x data and corresponding y data\n",
        "x_train = train_df.drop(labels=[\"price_range\"], axis=\"columns\")\n",
        "feature_names = x_train.columns.values\n",
        "x_train = x_train.values\n",
        "y_train = train_df[\"price_range\"].values\n",
        "\n",
        "####split validation data into x data and corresponding y data\n",
        "x_val = val_df.drop(labels=[\"price_range\"], axis=\"columns\")\n",
        "x_val = x_val.values\n",
        "y_val = val_df[\"price_range\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfzN3RSoMncB"
      },
      "source": [
        "## Question 2\n",
        "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
        "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
        "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2wWNDSEvMncB"
      },
      "outputs": [],
      "source": [
        "class decision_node: #store decision node in a tree\n",
        "  def __init__(self, less_branch, more_branch, question = None):\n",
        "    self.less_branch = less_branch\n",
        "    self.more_branch = more_branch\n",
        "    self.question = question\n",
        "class leaf_node: #store leaf node in a tree\n",
        "  def __init__(self, whole_data):\n",
        "    frequency = {}\n",
        "    for label in whole_data[:, -1]:\n",
        "      if label not in frequency:\n",
        "        frequency[label] = 0\n",
        "      frequency[label] +=1\n",
        "    max_key = 0\n",
        "    max_value = 0\n",
        "    for key, value in frequency.items():\n",
        "      if value > max_value:\n",
        "        max_key = key\n",
        "        max_value = value\n",
        "    self.final_class = max_key\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, criterion='gini', max_depth=None, boostrap=None, max_features=None):\n",
        "      self.depth = max_depth\n",
        "      self.cri = criterion\n",
        "      self.boostrap = boostrap\n",
        "      self.max_features = max_features\n",
        "\n",
        "    def fit(self, x_data, y_data, sample_weight = None):\n",
        "      whole_data = np.append(x_data, [[i] for i in y_data], 1)\n",
        "      if not sample_weight is None: \n",
        "        self.tree = build_tree(whole_data, self.cri, sample_weight, 0, 1) #adaboost\n",
        "      elif not self.boostrap is None:\n",
        "        if not self.max_features is None:\n",
        "          self.tree = build_tree(whole_data, self.cri, sample_weight, 0, -1, self.boostrap, self.max_features) #random forest\n",
        "      else:\n",
        "        self.tree = build_tree(whole_data, self.cri, sample_weight = None, current_depth = 0, target_depth = self.depth) #original DT\n",
        "        \n",
        "\n",
        "      \n",
        "\n",
        "    def predict(self, x_data):\n",
        "      pred = []\n",
        "      for target_index in range(len(x_data)):\n",
        "        data = x_data[target_index]\n",
        "        pred.append(classification(data, self.tree))          #given x predict y\n",
        "      prediction = np.array(pred)\n",
        "      return prediction\n",
        "\n",
        "class Question:         #store the feature and corresponding threshold of decision node\n",
        "  def __init__(self, feature, threshold):\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "  def split_branch(self, data):\n",
        "    less_index = []\n",
        "    more_index = []\n",
        "    for i in range(len(data)):\n",
        "      if data[i][self.feature] > self.threshold:\n",
        "        less_index.append(i)\n",
        "      else:\n",
        "        more_index.append(i)\n",
        "    return data[less_index], data[more_index]\n",
        "\n",
        "def build_tree(before_whole_data, cri, sample_weight = None, current_depth=0, target_depth=-1, boostrap=None, max_features=None):\n",
        "  largest_gain = 0\n",
        "  best_question = None\n",
        "  if current_depth == target_depth:\n",
        "    return leaf_node(before_whole_data)\n",
        "  if current_depth == 0:\n",
        "    if not sample_weight is None:\n",
        "      num = len(before_whole_data)\n",
        "      whole_data = before_whole_data[np.random.choice(np.arange(num), num, p = sample_weight)] #adaboost\n",
        "    elif not boostrap is None:\n",
        "      w= np.ones(len(before_whole_data)) * 1 / len(before_whole_data)\n",
        "      num = len(before_whole_data)\n",
        "      whole_data = before_whole_data[np.random.choice(np.arange(num), num, p = w)] #random forest\n",
        "    else:\n",
        "      whole_data = before_whole_data \n",
        "  else: whole_data = before_whole_data  \n",
        "  if not boostrap is None:\n",
        "    features = np.random.choice(len(whole_data[0])-1, int(max_features), replace=False) #random forest\n",
        "  else :  \n",
        "    features = np.arange(len(whole_data[0])-1)\n",
        "  \n",
        "  \n",
        "  if cri == 'gini':\n",
        "    \n",
        "    currrent_value = gini(whole_data[:, -1]) #gini\n",
        "  else:\n",
        "    currrent_value = entropy(whole_data[:, -1])  #entropy\n",
        "  for col_index in features:     #search for possible feature, threshold pair that can improve info gain\n",
        "    values = list(set(whole_data[:, col_index]))\n",
        "    if len(values) == 1: continue\n",
        "    for i in range(len(values) - 1):\n",
        "      mid_value = (values[i] + values[i + 1]) / 2\n",
        "      question = Question(col_index, mid_value)\n",
        "      less_data, more_data = question.split_branch(whole_data)\n",
        "      p = len(less_data) / (len(less_data) + len(more_data))\n",
        "      if cri == 'gini':\n",
        "        info_gain = currrent_value - p * gini(less_data[:, -1]) - (1 - p) * gini(more_data[:, -1])\n",
        "      else:\n",
        "        info_gain = currrent_value - p * entropy(less_data[:, -1]) - (1 - p) * entropy(more_data[:, -1])\n",
        "      if info_gain >= largest_gain:\n",
        "        largest_gain = info_gain\n",
        "        best_question = question\n",
        "  if largest_gain == 0:\n",
        "    return leaf_node(whole_data)\n",
        "  importance[best_question.feature] += 1 #count the importance of each feature\n",
        "  less_data, more_data = best_question.split_branch(whole_data)\n",
        "  less_branch = build_tree(less_data, cri, sample_weight, current_depth + 1, target_depth, boostrap, max_features)\n",
        "  more_branch = build_tree(more_data, cri, sample_weight, current_depth + 1, target_depth, boostrap, max_features)\n",
        "  return decision_node(less_branch, more_branch, best_question) # split the node into two branches according to the feature and threshold\n",
        "    \n",
        "def classification(data, node): #for classify data\n",
        "\n",
        "  if isinstance(node, leaf_node):\n",
        "    return node.final_class\n",
        "  if data[node.question.feature] > node.question.threshold:\n",
        "    return classification(data, node.less_branch)\n",
        "  else:\n",
        "    return classification(data, node.more_branch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShoZHGHpMncC"
      },
      "source": [
        "### Question 2.1\n",
        "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM-Z5YtFMncC",
        "outputId": "57c547e6-4f28-4d39-e82a-70a8530e142e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree\n",
            "max_depth = 3, accuracy score: 0.92\n",
            "max_depth = 10, accuracy score: 0.93\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
        "clf_depth3.fit(x_train, y_train)\n",
        "pred = clf_depth3.predict(x_val)\n",
        "print(\"Decision Tree\")\n",
        "print(\"max_depth = 3, accuracy score:\", accuracy_score(y_val, pred))\n",
        "\n",
        "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
        "clf_depth10.fit(x_train, y_train)\n",
        "pred = clf_depth10.predict(x_val)\n",
        "print(\"max_depth = 10, accuracy score:\", accuracy_score(y_val, pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxWApPiBMncD"
      },
      "source": [
        "### Question 2.2\n",
        "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc8f0jZGMncD",
        "outputId": "84964fcd-ebc6-4ab5-f908-9ae058f7b7d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree\n",
            "Criterion=‘gini', accuracy score: 0.92\n",
            "Criterion='entropy’, accuracy score: 0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
        "clf_gini.fit(x_train, y_train)\n",
        "pred = clf_gini.predict(x_val)\n",
        "print('Decision Tree')\n",
        "print(\"Criterion=‘gini', accuracy score:\",accuracy_score(y_val, pred))\n",
        "\n",
        "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
        "clf_entropy.fit(x_train, y_train)\n",
        "pred = clf_entropy.predict(x_val)\n",
        "print(\"Criterion='entropy’, accuracy score:\",accuracy_score(y_val, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs3DzKcAMncD"
      },
      "source": [
        "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
        "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
        "- Hint: You can use the recursive method to build the nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN7A5U2aMncD"
      },
      "source": [
        "## Question 3\n",
        "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
        "\n",
        "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
        "\n",
        "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "importance = [0 for i in range(20)]\n",
        "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
        "clf_depth10.fit(x_train, y_train)\n",
        "x = [i for i in range(20)]\n",
        "label = feature_names\n",
        "plt.barh(x, importance, tick_label = label,)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "QWEaet4chyty",
        "outputId": "52785ac6-0f74-4142-8cc3-a6454c9e094a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enchcjU3MncE"
      },
      "source": [
        "## Question 4\n",
        "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
        "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ICFYLYBUMncE"
      },
      "outputs": [],
      "source": [
        "class AdaBoost():\n",
        "    def __init__(self, n_estimators):\n",
        "      self.M = n_estimators\n",
        "      self.G_M = []\n",
        "      self.alphas = []\n",
        "      self.training_errors = []\n",
        "      self.prediction_errors = []\n",
        "\n",
        "    def fit(self, x_data, y_data):\n",
        "      for m in range(self.M):\n",
        "\n",
        "        if m == 0:\n",
        "          w = np.ones(len(y_data)) * 1 / len(y_data) \n",
        "        else:\n",
        "          w =  w * np.exp(alpha_m * (np.not_equal(y_data, y_pred)).astype(int)) #update w\n",
        "          w /= w.sum()                      #sum of w must be 1\n",
        "        weak_tree = DecisionTree(max_depth = 1) \n",
        "        \n",
        "        weak_tree.fit(x_data, y_data, sample_weight = w)\n",
        "        y_pred = weak_tree.predict(x_data)\n",
        "        self.G_M.append(weak_tree) # record stump\n",
        "        error_m = (sum(w * (np.not_equal(y_data, y_pred)).astype(int)))/sum(w)\n",
        "        \n",
        "        self.training_errors.append(error_m)\n",
        "        alpha_m = np.log((1 - error_m) / error_m) #record alpha\n",
        "\n",
        "        self.alphas.append(alpha_m)\n",
        "\n",
        "      assert len(self.G_M) == len(self.alphas)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, x_data): #predict according to recorded stumps and alphas\n",
        "      pred = []\n",
        "      for i in range(len(x_data)):\n",
        "        data = [x_data[i]]\n",
        "        judge = 0\n",
        "        for m in range(self.M):\n",
        "          if self.G_M[m].predict(data)[0] == 1 :\n",
        "            judge += self.alphas[m]\n",
        "          else:\n",
        "            judge -= self.alphas[m]\n",
        "        if judge > 0:\n",
        "          pred.append(1)\n",
        "        else:\n",
        "          pred.append(0)\n",
        "      prediction = np.array(pred)\n",
        "      return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "1z7143WNMncE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "ada10 = AdaBoost(10)\n",
        "ada10.fit(x_train, y_train)\n",
        "pred10 = ada10.predict(x_val)\n",
        "\n",
        "ada100 = AdaBoost(100) #1000 for best\n",
        "ada100.fit(x_train, y_train)\n",
        "pred100 = ada100.predict(x_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51yzJE9cMncE"
      },
      "source": [
        "### Question 4.1\n",
        "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VrHCNHrMncF",
        "outputId": "d966893c-a5b0-4fb6-d761-43fdee442bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBooest\n",
            "n_estimators=10, accuracy score: 0.9366666666666666\n",
            "n_estimators=100, accuracy score: 0.9733333333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('AdaBooest')\n",
        "print(\"n_estimators=10, accuracy score:\",accuracy_score(y_val, pred10))\n",
        "print(\"n_estimators=100, accuracy score:\",accuracy_score(y_val, pred100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAOllT4pMncF"
      },
      "source": [
        "## Question 5\n",
        "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
        "\n",
        "1. **n_estimators**: The number of trees in the forest. \n",
        "2. **max_features**: The number of random select features to consider when looking for the best split\n",
        "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "CCfQiHSNMncF"
      },
      "outputs": [],
      "source": [
        "class RandomForest():\n",
        "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
        "      self.n_estimators = n_estimators\n",
        "      self.max_features = max_features\n",
        "      self.boostrap = boostrap\n",
        "      self.cri = criterion\n",
        "      self.max_depth = max_depth\n",
        "      self.G_M = []\n",
        "\n",
        "    def fit(self, x_data, y_data):\n",
        "      for m in range(self.n_estimators):\n",
        "        random_tree = DecisionTree(boostrap=self.boostrap, max_features = self.max_features)\n",
        "        random_tree.fit(x_data, y_data)\n",
        "        self.G_M.append(random_tree) #record random trees\n",
        "\n",
        "\n",
        "    def predict(self, x_data): #predict y according to recored trees\n",
        "      pred = []\n",
        "      for i in range(len(x_data)):\n",
        "        data = [x_data[i]]\n",
        "        judge = 0\n",
        "        for m in range(self.n_estimators):\n",
        "          if self.G_M[m].predict(data)[0] == 1 :  #vote for class 0 or 1\n",
        "            judge += 1\n",
        "          else:\n",
        "            judge -= 1\n",
        "        if judge >= 0:\n",
        "          pred.append(1)\n",
        "        else:\n",
        "          pred.append(0)\n",
        "      prediction = np.array(pred)\n",
        "      return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40QaMRYsMncF"
      },
      "source": [
        "### Question 5.1\n",
        "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vHUK13HvMncF"
      },
      "outputs": [],
      "source": [
        "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
        "clf_10tree.fit(x_train, y_train)\n",
        "pred10_ran = clf_10tree.predict(x_val)\n",
        "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]))\n",
        "clf_100tree.fit(x_train, y_train)\n",
        "pred100_ran = clf_100tree.predict(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gly7QU_UMncG",
        "outputId": "1efaaa23-3e5d-4dce-b0d1-6c8e7feef5cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest\n",
            "n_estimators=10, accuracy score: 0.93\n",
            "n_estimators=100, accuracy score: 0.96\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print(\"n_estimators=10, accuracy score:\",accuracy_score(y_val, pred10_ran))\n",
        "print(\"n_estimators=100, accuracy score:\",accuracy_score(y_val, pred100_ran))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1cHOLivMncG"
      },
      "source": [
        "### Question 5.2\n",
        "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Rf2KRUzOMncG"
      },
      "outputs": [],
      "source": [
        "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
        "clf_random_features.fit(x_train, y_train)\n",
        "pred_ran = clf_random_features.predict(x_val)\n",
        "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])\n",
        "clf_all_features.fit(x_train, y_train)\n",
        "pred_all = clf_all_features.predict(x_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiKotPfcMncG"
      },
      "source": [
        "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS5PkRl-MncG",
        "outputId": "3c177075-9b89-4e44-a264-3af79c788dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest\n",
            "random features, accuracy score: 0.9333333333333333\n",
            "all features, accuracy score: 0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print(\"random features, accuracy score:\",accuracy_score(y_val, pred_ran))\n",
        "print(\"all features, accuracy score:\",accuracy_score(y_val, pred_all))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us-jfv05MncG"
      },
      "source": [
        "### Question 6. Train and tune your model on a real-world dataset\n",
        "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
        "- Feature engineering\n",
        "- Hyperparameter tuning\n",
        "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "N-ggStKSMncG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_your_model(data):\n",
        "    ## Define your model and training \n",
        "    x_train = data.drop(labels=[\"price_range\"], axis=\"columns\")\n",
        "    x_train = x_train.values\n",
        "    y_train = data[\"price_range\"].values\n",
        "\n",
        "    ada = 150\n",
        "    adatest = AdaBoost(ada)\n",
        "    adatest.fit(x_train, y_train)\n",
        "    predtest = adatest.predict(x_val)\n",
        "    return adatest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "57JHs5PvMncG"
      },
      "outputs": [],
      "source": [
        "trainval_df = train_df.append(val_df)\n",
        "my_model = train_your_model(trainval_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "kgiOlwLlMncH"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('x_test.csv')\n",
        "x_test = test_df.values\n",
        "y_pred = ada100.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "bour7-n9MncH"
      },
      "outputs": [],
      "source": [
        "assert y_pred.shape == (500, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmAyet78MncH"
      },
      "source": [
        "## Supplementary\n",
        "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw48cQMHMncH"
      },
      "source": [
        "### DO NOT MODIFY CODE BELOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mTR9iBrMncH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
        "\n",
        "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcMsGAHCMncH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfXbq3i3MncI",
        "outputId": "a4c6d491-a814-4f8d-d7cf-0a8b7c8f3abe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** We will check your result for Question 3 manually *** (5 points)\n",
            "*** We will check your result for Question 6 manually *** (20 points)\n",
            "Approximate score range: 45.0 ~ 70.0\n",
            "*** This score is only for reference ***\n"
          ]
        }
      ],
      "source": [
        "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict(x_test)\n",
        "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
        "        return score\n",
        "    else:\n",
        "        print(f\"{name} failed\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "def patient_checker(score, thres, CLS, kwargs, name,\n",
        "                    x_train, y_train, x_test, y_test, patient=10):\n",
        "    while patient > 0:\n",
        "        patient -= 1\n",
        "        clf = CLS(**kwargs)\n",
        "        clf.fit(x_train, y_train)\n",
        "        y_pred = clf.predict(x_test)\n",
        "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
        "            return score\n",
        "    print(f\"{name} failed\")\n",
        "    print(\"Considering the randomness, we will check it manually\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
        "    df = pd.read_csv(\n",
        "        file_url,\n",
        "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
        "    )\n",
        "\n",
        "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
        "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
        "\n",
        "    train_idx = range(0, len(df), 10)\n",
        "    test_idx = range(1, len(df), 20)\n",
        "\n",
        "    train_df = df.iloc[train_idx]\n",
        "    test_df = df.iloc[test_idx]\n",
        "\n",
        "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
        "    feature_names = x_train.columns.values\n",
        "    x_train = x_train.values\n",
        "    y_train = train_df['Target'].values\n",
        "\n",
        "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
        "    x_test = x_test.values\n",
        "    y_test = test_df['Target'].values\n",
        "    return x_train, y_train, x_test, y_test, feature_names\n",
        "\n",
        "\n",
        "score = 0\n",
        "\n",
        "data = np.array([1, 2])\n",
        "if abs(gini(data) - 0.5) < 1e-4:\n",
        "    score += 2.5\n",
        "else:\n",
        "    print(\"gini test failed\")\n",
        "\n",
        "if abs(entropy(data) - 1) < 1e-4:\n",
        "    score += 2.5\n",
        "else:\n",
        "    print(\"entropy test failed\")\n",
        "\n",
        "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
        "\n",
        "score += discrete_checker(5, 0.9337,\n",
        "                          DecisionTree(criterion='gini', max_depth=3),\n",
        "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "score += discrete_checker(2.5, 0.9036,\n",
        "                          DecisionTree(criterion='gini', max_depth=10),\n",
        "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "score += discrete_checker(2.5, 0.9096,\n",
        "                          DecisionTree(criterion='entropy', max_depth=3),\n",
        "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
        "\n",
        "score += patient_checker(\n",
        "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
        "    \"AdaBoost(n_estimators=10)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
        "    \"AdaBoost(n_estimators=100)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.91, RandomForest,\n",
        "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
        "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.91, RandomForest,\n",
        "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
        "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.92, RandomForest,\n",
        "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
        "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
        "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
        "print(\"*** This score is only for reference ***\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c9afcc29181526987d0b385fb14ac7bec129a3a1f159e207d0db8e842e65205"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}